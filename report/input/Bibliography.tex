\begin{thebibliography}{11}

\bibitem{ttbaremu} \textit{Measurement of jet activity produced in top-quark events with an electron, a muon and two b-tagged jets in the final state in pp collision at s=13 TeV with the ATLAS detector}, \textbf{arXiv}: 1610.09978v2, \url{https://arxiv.org/pdf/1610.09978.pdf}
\bibitem{ATLAS} The ATLAS experiment, \url{https://atlas.cern}
\bibitem{RootFile} The \ttbaremu~ROOT file used {\tiny \texttt{\detokenize{/afs/cern.ch/user/l/lciucu/public/data/MLUnfolding/user.yili.18448069._000001.output.sync.root}}}
\bibitem{UnfoldingStatSchool}, Stefan Schmitt, Daniel Brizger, \textit{Slides Unfolding in High Energy Physics at the Terascale statistics school 2014}, \url{http://www.desy.de/~sschmitt/talks/UnfoldStatSchool2014.pdf?fbclid=IwAR3CB3CwJOQchKfbZRIg6ktRf6DbI5KTD5cdtBbNHS1lKb-q3yrwagyua5w}.
\bibitem{ReportYichenLi} Yichen Li, \textit{ZUnfold framework, an unfolding framework written at Zeuthen}, \url{http://www.desy.de/~liyichen/Unfolding.pdf}
\bibitem{AGlazov} Alexander Glazov, \textit{Machine learning as an instrument for data unfolding}, \textbf{arXiv}: 1712.01814v1, \url{http://inspirehep.net/record/1641082}
\bibitem{AGlazovCode} Alexander Glazov, \textit{Toy data example for machine learning as an instrument  for data unfolding using TensorFlow via Keras in a Python Jupyter Notebook}, \url{https://github.com/aglazov/MLUnfold/blob/master/Unfold.ipynb}
\bibitem{TensorFlow} The TensorFlow machine learning library, \url{https:// www.tensforflow.org}
\bibitem{Keras} The keras machine learning library, \url{https://keras.io}
\bibitem{numpy} The Numerical Python library, \url{https://www.numpy.org}
\bibitem{SWAN} The SWAN server at CERN, \url{https://swan003.cern.ch}
\bibitem{singularity} The singularity command to run the ML environment, \\ {\tiny \texttt {\detokenize{singularity exec '/cvmfs/unpacked.cern.ch/registry.hub.docker.com/atlasml/ml-base:latest'bash}}}
\bibitem{uproot} The uproot package to read .root files in numpy arrays in Python, \url{https://github.com/scikit-hep/uproot}
\bibitem{JoshuaBendavid} Joshua Bendavid, \textit{Efficient Monte Carlo Integration Using Boosted Decision Trees and Generative Deep Neural Networks}, 2017
\bibitem{AndrewNg} Andrew Ng, \textit{Machine Learning. Coursera online course}, \url{URL https://www.coursera.org/learn/machine learning}
\bibitem{OReilly} Adam Gibson, Josh Patterson, \texttt{Deep Learning}, book published at O'Reilly, \url{https://www.oreilly.com/library/view/deep-learning/9781491924570/ch04.html}
\bibitem{TheFork} TheFork.com, \textit{Activation functions}, \url{https://theffork.com/activation-functions-in-neural-networks/}
\bibitem{softmax} Uniqtech, \textit{Understand the Softmax Function in Minutes}, article on Medium.com, \url{https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d}
\bibitem{ReLU} Jason Brownlee, \textit{A Gentle Introduction to the Rectified Linear Unit (ReLU)}, \url{https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/}









\end{thebibliography}
