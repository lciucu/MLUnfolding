\section{Neural Networks}
\label{sec:NeuralNetworks}

The goal of this project is to adapt a code example of machine learning unfolding (MLUnfolding) using toy data to use it for the jet energy reconstruction of \ttbaremu~analysis.

\ \\In this project one of the problems that we want to solve is the correction of detector smearing using Machine Learning (ML). I have implemented this method using a sequential Neural Network. The unfolding corresponds to an inversion of the migration matrix. Unfolding is the procedure to infer the truth data (what happens in reality in nature) from the reconstructed data (observed and measured in our experiment by our detector). The categorisation is a common problem for the modern Machine Learning methods. Possible methods could be Boosted Decision Trees (BDT) and Artificial Neural Networks (ANN), or in short Neural Networks (NN). NNs with various architectures can be tried for the unfolding problem. The unfolding involves several iterative steps of ML using a NN training in TensorFlow via Keras, in Python. The NN takes as input the reconstructed values, and has as output the truth values.

\ \\The goal of the study is to adapt this example to use real data coming from the jet \pt~distribution from the \ttbaremu~analysis. We use this \texttt{.root} flat tree~\cite{RootFile}. This file contains both the reconstructed and truth jets, already matched. Meaning the i$^{\rm th}$ event from reco, corresponds to the i$^{\rm th}$ event from truth. Each jet collection contains jets already the jets ranked by \pt. We take index 0 to consider the leading reco jet and the leading truth jet.

\ \\Given the leading jet \pt~as a continuous reco value, we want to find out in which jet \pt~bin does the truth jet falls. The jet bin is configurable, say 10 \GeV, or 20 \GeV. The input is a continuous value, but the output is a discrete value, that can take values from 0 to 24 if we consider 20 \GeV~bins from 0 to 500 \GeV. For example, bin index 0 contains jets with \pt~from 0 to 20 \GeV, bin index 1 contains jets with \pt~from 20 \GeV~to 40 \GeV, \emph{etc.}. Jets with \pt~larger than 500 \GeV~have values set by hand to 499.999 \GeV, and enter in the last possible bin index, number 24. It is equivalent to moving the overflow bin of a histogram to the last bin of the histogram. 

\ \\Given the output can take any value from 0 to 24, the problem we try to solve is a classification, and the possible labels are not only two (as in a simple signal to background classification, typically used in ATLAS), but a more complex one, with 25 labels. The NN will return the probability that for a given jet its \pt~falls in any of these bins. The total probability for all bins must be 1.0. This is ensured by the \emph{softmax} activation function for the last layer of the NN. We then consider the bin with the largest probability as our choice by the NN. The predicted bin value can then be compared with the true bin value when making the plots of this project.

\ \\To optimize the NN performance, we take as input in fact not the jet \pt, but the jet \pt~divided by the bin width. This is the jet \pt~bin as a real value. Making things more consistent with the output being the integer jet \pt~bin value for the truth.

\ \\ Neural Networks are an example of Machine Learning. In this way the computers learn a solution for a problem without being explicitly programmed. The two main classes of ML are supervised and unsupervised. 
In this project I used supervised ML. ......~\cite{AndrewNg}.

\ \\If we have the function Pi(y), a multidimensional highly non-linear, an efficient way to do this procedure is with an NN. This was inspired by the brain structure, which contains millions of neurone cells forming a network with electrochemical impulses passing between them. An artificial neural network formed by a number of interconnected artificial \emph{neurons}, or "nodes" respects this architecture. 

\ \\A network is formed by several ayers of nodes connected in series. Each node takes a weighted linear combination of the outputs from nodes of the previous layer, applies "activation function", then outputs the result do the next layer~\cite{AndrewNg}.

\ \\Using a "loss" function we can describe the difference between the predicted and real outputs, like the mean squared error between real and predicted. The weights associated with each node is modifies via an algorithm, and from here we can deduce that the loss function decreases and training increases.

\ \\We know from The \emph{Universal Approximation Theorem} that a neural network with one \emph{hidden layer} of nodes between input and output can in principle approximate any N-dimensional function to an arbitrary degree of accuracy, given a sufficiently large (though finite) number of nodes. 

\ \\In practice it is more suitable to use multiple hidden layers connected in series~\cite{AndrewNg}.
